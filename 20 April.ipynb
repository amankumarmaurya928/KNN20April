{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed5c87ee-003b-4c00-b537-bf22ff478d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KNN is one of the simplest forms of machine learning algorithms mostly used for classification. It classifies the data \\n   point on how its neighbor is classified. KNN classifies the new data points based on the similarity measure of the earlier\\n   stored data points. For example, if we have a dataset of tomatoes and bananas.\\n   '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1\n",
    "'''KNN is one of the simplest forms of machine learning algorithms mostly used for classification. It classifies the data \n",
    "   point on how its neighbor is classified. KNN classifies the new data points based on the similarity measure of the earlier\n",
    "   stored data points. For example, if we have a dataset of tomatoes and bananas.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cb9d3d8-7a74-4b4d-b240-5af69b5a6b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with \\n   higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and \\n   cross-validation tactics can help you choose the optimal k for your dataset.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2\n",
    "'''The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with \n",
    "   higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and \n",
    "   cross-validation tactics can help you choose the optimal k for your dataset.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13ac8339-4fa6-449e-bfc9-0053c1715d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The difference between the KNN classifier and KNN regression methods is that the classifier is used in situations where the\\n   response variable is categorical (qualitative), \\n   while the regressor is used in numerical situations (quantitative).\\n   KNN regression tries to predict the value of the output variable by using a local average.\\n   KNN classification attempts to predict the class to which the output variable belong by computing the local probability.\\n   '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3\n",
    "'''The difference between the KNN classifier and KNN regression methods is that the classifier is used in situations where the\n",
    "   response variable is categorical (qualitative), \n",
    "   while the regressor is used in numerical situations (quantitative).\n",
    "   KNN regression tries to predict the value of the output variable by using a local average.\n",
    "   KNN classification attempts to predict the class to which the output variable belong by computing the local probability.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f625d71b-d980-4294-91e9-e98c0f31ad13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main concept for k-NN depends on calculating the distances between the tested, and the training data samples in order \\n   to identify its nearest neighbours. The tested sample is then simply assigned to the class of its nearest neighbour.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4\n",
    "'''The main concept for k-NN depends on calculating the distances between the tested, and the training data samples in order \n",
    "   to identify its nearest neighbours. The tested sample is then simply assigned to the class of its nearest neighbour.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9cdbf5a-1914-4fc4-b78a-45bcf6ff9542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Curse of Dimensionality is a tongue in cheek way of stating that there's a ton of space in high-dimensional data sets.\\n   The size of the data space grows exponentially with the number of dimensions. This means that the size of your data set must\\n   also grow exponentially in order to keep the same density.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5\n",
    "'''The Curse of Dimensionality is a tongue in cheek way of stating that there's a ton of space in high-dimensional data sets.\n",
    "   The size of the data space grows exponentially with the number of dimensions. This means that the size of your data set must\n",
    "   also grow exponentially in order to keep the same density.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f507c78f-15c0-493a-930c-da6905f3a36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The idea in kNN methods is to identify 'k' samples in the dataset that are similar or close in the space. Then we use these\\n   'k' samples to estimate the value of the missing data points. Each sample's missing values are imputed using the mean value\\n   of the 'k'-neighbors found in the dataset.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6\n",
    "'''The idea in kNN methods is to identify 'k' samples in the dataset that are similar or close in the space. Then we use these\n",
    "   'k' samples to estimate the value of the missing data points. Each sample's missing values are imputed using the mean value\n",
    "   of the 'k'-neighbors found in the dataset.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1425414b-4dca-4223-a41c-88a28932a20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The difference between the KNN classifier and KNN regression methods is that the classifier is used in situations where the\\n   response variable is categorical (qualitative), while the regressor is used in numerical situations (quantitative).\\n   Regression algorithms solve regression problems such as house price predictions and weather predictions. Classification\\n   algorithms solve classification problems like identifying spam e-mails, spotting cancer cells, and speech recognition.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7\n",
    "'''The difference between the KNN classifier and KNN regression methods is that the classifier is used in situations where the\n",
    "   response variable is categorical (qualitative), while the regressor is used in numerical situations (quantitative).\n",
    "   Regression algorithms solve regression problems such as house price predictions and weather predictions. Classification\n",
    "   algorithms solve classification problems like identifying spam e-mails, spotting cancer cells, and speech recognition.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fc10097-e9d5-4d88-9c66-cebc4f87e02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's main disadvantages are that it is quite computationally inefficient and its difficult to pick the “correct” value of K.\\n   However, the advantages of this algorithm is that it is versatile to different calculations of proximity, it's very\\n   intuitive and that it's a memory based approach.\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q8\n",
    "'''It's main disadvantages are that it is quite computationally inefficient and its difficult to pick the “correct” value of K.\n",
    "   However, the advantages of this algorithm is that it is versatile to different calculations of proximity, it's very\n",
    "   intuitive and that it's a memory based approach.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58e3c52d-f60c-467f-942a-357d49055342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Euclidean distance is the shortest path between source and destination which is a straight line.\\n   but Manhattan distance is sum of all the real distances between source(s) and destination(d) and each distance are always\\n   the straight lines.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q9\n",
    "'''Euclidean distance is the shortest path between source and destination which is a straight line.\n",
    "   but Manhattan distance is sum of all the real distances between source(s) and destination(d) and each distance are always\n",
    "   the straight lines.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb770866-d9db-49e4-80a4-fb056313800d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Feature scaling is essential for machine learning algorithms that calculate distances between data. If not scaled, the\\n   feature with a higher value range starts dominating when calculating distances. KNN which uses Euclidean distance is one\\n   such algorithm which essentially require scaling.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q10\n",
    "'''Feature scaling is essential for machine learning algorithms that calculate distances between data. If not scaled, the\n",
    "   feature with a higher value range starts dominating when calculating distances. KNN which uses Euclidean distance is one\n",
    "   such algorithm which essentially require scaling.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd922b6d-780a-40c2-acb8-4bf35721908f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
